# gradient_boosting_clustering
Supervised learning which XGBoost is applied to requires labelled data. XGBoost can be applied to both classification problems and regression problems. It works with both binary and multi-class classification. The evaluation metrics usually used include: accuracy, AUC, and confusion matrix. 
XGBoost refers to optimized gradient boosting machine learning library. It is very popular due to its speed and performance, core algorithm is parallelizable allowing it to be trained on lots and lots of data, consistently outperforms single-algorithm methods, and gives a state-of-the-art performance with ML tasks. 

Unsupervised learning is a group of machine learning algorithms used to find patterns in data. Data for algorithms has not been labeled, classified, or characterized. The objective of the algorithm is to interpret any structure in the data. Common unsupervised learning algorithms are: clustering, neural networks, and anomaly detection. 

Clustering is the process of grouping items with similar characteristics. Items in groups similar to each other than in other groups.
Until it identifies the ideal centroid, the K-means clustering method computes the centroids. The quantity of clusters is presumed to be known. The flat clustering algorithm is another name for it. K in K-means stands for the number of clusters that an algorithm has found in the data.
